{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Detection\n",
    "# Forrester Welch\n",
    "# The goal of this project to recognize where an instance of text appears in an image\n",
    "\n",
    "# Imports for convolutional neural networks, data management, and image processing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.layers as layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the cocotext json file \n",
    "# Download the cocotext annotations json here: https://bgshih.github.io/cocotext/#h2-download\n",
    "# Download cocotext.v2.zip [12 MB] and unzip for cocotext.v2.json, then rename to cocotext.json\n",
    "data = pd.read_json('cocotext.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236291 entries, 45346 to 390310\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   cats       0 non-null       float64\n",
      " 1   anns       201126 non-null  object \n",
      " 2   imgs       53686 non-null   object \n",
      " 3   imgToAnns  53686 non-null   object \n",
      " 4   info       0 non-null       float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 10.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cats', 'anns', 'imgs', 'imgToAnns', 'info'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 67.21,\n",
       " 'bbox': [262.6, 218.4, 9.8, 8.1],\n",
       " 'class': 'machine printed',\n",
       " 'id': 102540,\n",
       " 'image_id': 353906,\n",
       " 'language': 'english',\n",
       " 'legibility': 'illegible',\n",
       " 'mask': [263.5, 219.3, 262.6, 225.9, 272.4, 226.5, 272.0, 218.4],\n",
       " 'utf8_string': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of element in data['anns']\n",
    "data['anns'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through annotations\n",
    "# only add elements if machine printed, english, and legible\n",
    "# create dataset of image ids (we will later convert id to image filename)\n",
    "# create bbox dataset\n",
    "annotations = data['anns']\n",
    "image = []\n",
    "bbox = []\n",
    "\n",
    "# This step may take a couple minutes\n",
    "for i in range(len(data['anns'])):\n",
    "    current = annotations.iloc[i]\n",
    "    if(pd.isna(current)):\n",
    "        continue\n",
    "    if(current['class'] == 'machine printed' and current['language'] == 'english'\n",
    "      and current['legibility'] == 'legible' and current['image_id'] not in list(image)):\n",
    "            image.append(annotations.iloc[i]['image_id'])\n",
    "            bbox.append(annotations.iloc[i]['bbox'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COCO_train2014_000000102540.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example element of data['imgs']\n",
    "data['imgs'].iloc[1000]['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][('gfg@example.com', 'some value')]\n",
      "\n",
      "[('portal@example.com', 'some other value')][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][('gfg@example.com', 'some value')]\n",
      "\n",
      "some other value\n",
      "\n",
      "[][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][][('gfg@example.com', 'some value')]\n"
     ]
    }
   ],
   "source": [
    "# To change the image_id value to the filename of the image, I need a hashmap of key-value pairs\n",
    "# The dict object in python is supposed to operate like a hash map, but I could not figure out how\n",
    "# to make it work for our purposes. I found this implementation of a hash table at the link listed\n",
    "# below. This HashTable implementation made it simple and easy to convert image_id to image name.\n",
    "# https://www.geeksforgeeks.org/hash-map-in-python/\n",
    "\n",
    "class HashTable: \n",
    "  \n",
    "    # Create empty bucket list of given size \n",
    "    def __init__(self, size): \n",
    "        self.size = size \n",
    "        self.hash_table = self.create_buckets() \n",
    "  \n",
    "    def create_buckets(self): \n",
    "        return [[] for _ in range(self.size)] \n",
    "  \n",
    "    # Insert values into hash map \n",
    "    def set_val(self, key, val): \n",
    "        \n",
    "        # Get the index from the key \n",
    "        # using hash function \n",
    "        hashed_key = hash(key) % self.size \n",
    "          \n",
    "        # Get the bucket corresponding to index \n",
    "        bucket = self.hash_table[hashed_key] \n",
    "  \n",
    "        found_key = False\n",
    "        for index, record in enumerate(bucket): \n",
    "            record_key, record_val = record \n",
    "              \n",
    "            # check if the bucket has same key as \n",
    "            # the key to be inserted \n",
    "            if record_key == key: \n",
    "                found_key = True\n",
    "                break\n",
    "  \n",
    "        # If the bucket has same key as the key to be inserted, \n",
    "        # Update the key value \n",
    "        # Otherwise append the new key-value pair to the bucket \n",
    "        if found_key: \n",
    "            bucket[index] = (key, val) \n",
    "        else: \n",
    "            bucket.append((key, val)) \n",
    "  \n",
    "    # Return searched value with specific key \n",
    "    def get_val(self, key): \n",
    "        \n",
    "        # Get the index from the key using \n",
    "        # hash function \n",
    "        hashed_key = hash(key) % self.size \n",
    "          \n",
    "        # Get the bucket corresponding to index \n",
    "        bucket = self.hash_table[hashed_key] \n",
    "  \n",
    "        found_key = False\n",
    "        for index, record in enumerate(bucket): \n",
    "            record_key, record_val = record \n",
    "              \n",
    "            # check if the bucket has same key as  \n",
    "            # the key being searched \n",
    "            if record_key == key: \n",
    "                found_key = True\n",
    "                break\n",
    "  \n",
    "        # If the bucket has same key as the key being searched, \n",
    "        # Return the value found \n",
    "        # Otherwise indicate there was no record found \n",
    "        if found_key: \n",
    "            return record_val \n",
    "        else: \n",
    "            return \"No record found\"\n",
    "  \n",
    "    # Remove a value with specific key \n",
    "    def delete_val(self, key): \n",
    "        \n",
    "        # Get the index from the key using \n",
    "        # hash function \n",
    "        hashed_key = hash(key) % self.size \n",
    "          \n",
    "        # Get the bucket corresponding to index \n",
    "        bucket = self.hash_table[hashed_key] \n",
    "  \n",
    "        found_key = False\n",
    "        for index, record in enumerate(bucket): \n",
    "            record_key, record_val = record \n",
    "              \n",
    "            # check if the bucket has same key as \n",
    "            # the key to be deleted \n",
    "            if record_key == key: \n",
    "                found_key = True\n",
    "                break\n",
    "        if found_key: \n",
    "            bucket.pop(index) \n",
    "        return\n",
    "  \n",
    "    # To print the items of hash map \n",
    "    def __str__(self): \n",
    "        return \"\".join(str(item) for item in self.hash_table) \n",
    "  \n",
    "  \n",
    "hash_table = HashTable(50) \n",
    "  \n",
    "# insert some values \n",
    "hash_table.set_val('gfg@example.com', 'some value') \n",
    "print(hash_table) \n",
    "print() \n",
    "  \n",
    "hash_table.set_val('portal@example.com', 'some other value') \n",
    "print(hash_table) \n",
    "print() \n",
    "  \n",
    "# search/access a record with key \n",
    "print(hash_table.get_val('portal@example.com')) \n",
    "print() \n",
    "  \n",
    "# delete or remove a value \n",
    "hash_table.delete_val('portal@example.com') \n",
    "print(hash_table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize HashTable with key-value pair of image_id <-> file_name\n",
    "\n",
    "# Our hashtable that will store key-value pairs of id-filename\n",
    "image_tree = HashTable(10000)\n",
    "\n",
    "# Cycle through imgs to gather data\n",
    "for i in range(len(data['imgs'])):\n",
    "    if(pd.isna(data['imgs'].iloc[i])):\n",
    "        continue\n",
    "    current = data['imgs'].iloc[i]\n",
    "    image_tree.set_val(current['id'], current['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image vector to contain file names instead of id numbers\n",
    "for i in range(len(image)):\n",
    "    image[i] = image_tree.get_val(image[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code can be skipped in the future. It is now commented out for final submission\n",
    "# The purpose of this block is to put the image file names into a text file\n",
    "# The reason for this has to do with how the images for this project were collected.\n",
    "# The coco-text.json annotations were released as an addendum to the original COCO2014 image dataset.\n",
    "# Every instance of text in that datset was recorded in coco-text.json. However, not every image\n",
    "# has an instance of text. It is not posssible to download just the text images, only the entire\n",
    "# 2014 COCO image dataset can be downloaded. To save storage space, I moved the necessary images out\n",
    "# of the folder so I could delete the unnecessary images all at once. For reference, the terminal\n",
    "# command to move a list of files is as follows: \n",
    "# for i in $(cat all_text_image_names.txt); do mv \"$i\" /temp_dest/; done\n",
    "# The set of necessary images can be found in my github at: \n",
    "\n",
    "\n",
    "#unique_filenames = list(set(image))\n",
    "#import numpy as np\n",
    "#name_file = open(\"all_text_image_names.txt\", \"w\")\n",
    "#np.savetxt(name_file, unique_filenames, fmt=\"%s\")\n",
    "\n",
    "#name_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of image file_names to a list of 2d-arrays of pixels\n",
    "# Converts the bbox into a scale of [0,1]\n",
    "# bbox is originally annotated: x,y,width,height\n",
    "    # We convert to xmin, ymin, xmax, ymax on scale of 0-1\n",
    "# This step may take five minutes\n",
    "for i in range(len(image)):\n",
    "    image_name = \"train2014/\" + image[i]\n",
    "    width, height = Image.open(image_name).size\n",
    "    xmax = (bbox[i][0] + bbox[i][2]) / width\n",
    "    ymax = (bbox[i][1] + bbox[i][3]) / height\n",
    "    xmin = bbox[i][0] / width\n",
    "    ymin = bbox[i][1] / height\n",
    "    bbox[i] = [xmin, ymin, xmax, ymax]\n",
    "    # The images are resized to (100,100) because the kernel could not handle a larger size\n",
    "    # With limitless computational resources, a full size 600x600 image may yield more accurate results\n",
    "    # Interestingly, when images were resized to 128x128 or 164x164, they had slightly less accuracy\n",
    "    # than the 100x100 option.\n",
    "    file = tf.keras.preprocessing.image.load_img(image_name, target_size=(100,100), color_mode='grayscale')\n",
    "    image[i] = tf.keras.preprocessing.image.img_to_array(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image pixel values are rescaled from [0-1]\n",
    "image = np.array(image, dtype=\"float32\") / 255\n",
    "bbox = np.array(bbox, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test data with split size .2 and random seed = 400\n",
    "image_train, image_test, bbox_train, bbox_test = train_test_split(image, bbox, test_size=0.2, random_state=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imgs to create key value map of id to image name\n",
    "# get list of all image names from imgs\n",
    "# cycle through anns and keep every image that contains an annotation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a keras Model\n",
    "# Our model has 3 convolutional layers with 32,32, and 64 filters \n",
    "# The final layer of our model returns four neurons, each representing a coordinate of the bounding box\n",
    "# This architecture differs from traditional neural networks doing classification\n",
    "# Instead of recognizing what an object is, we aim to find where an object is\n",
    "# This is done using regression to calculate the best fitting bounding box.\n",
    "# The outline for how to make a regression layer the final layer of the model was found at the \n",
    "# following link: https://medium.com/analytics-vidhya/object-localization-with-keras-2f272f79e03c\n",
    "# We trained the model using a different number of filters on each layer, as well as a different number\n",
    "# of layers. The results of these experiments are noted in the final writeup. Ultimately, having \n",
    "# more filters lead to overfitting which lowered accuracy on the validation datset.\n",
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(100,100,1))\n",
    "    x = layer.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "    x = layer.MaxPooling2D((3,3))(x)\n",
    "    x = layer.Conv2D(32, (3,3), activation='relu')(x)\n",
    "    x = layer.MaxPooling2D((3,3))(x)\n",
    "    x = layer.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = layer.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    \n",
    "    reg_head = layer.Dense(128, activation='relu')(x)\n",
    "    reg_head = layer.Dense(64, activation='relu')(x)\n",
    "    reg_head = layer.Dense(32, activation='relu')(reg_head)\n",
    "    # Notice the name of the layer.\n",
    "    reg_head = layer.Dense(4, activation='sigmoid', name='bbox')(reg_head)\n",
    "    return tf.keras.Model(inputs=[inputs], outputs=[reg_head])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "bbox (Dense)                 (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 34,436\n",
      "Trainable params: 34,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initalize the model\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# We experimented with more epochs, but this led to overfitting the data and lowered the accuracy\n",
    "# on the validation set. Around 30 epochs, the loss function comes close to convergence.\n",
    "epochs = 30\n",
    "\n",
    "losses = \"mean_squared_error\"\n",
    "\n",
    "model.compile(loss=losses, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "83/83 [==============================] - 60s 704ms/step - loss: 0.0702 - accuracy: 0.5224 - val_loss: 0.0702 - val_accuracy: 0.5308\n",
      "Epoch 2/30\n",
      "83/83 [==============================] - 53s 637ms/step - loss: 0.0698 - accuracy: 0.5219 - val_loss: 0.0701 - val_accuracy: 0.5308\n",
      "Epoch 3/30\n",
      "83/83 [==============================] - 54s 645ms/step - loss: 0.0689 - accuracy: 0.5336 - val_loss: 0.0691 - val_accuracy: 0.5308\n",
      "Epoch 4/30\n",
      "83/83 [==============================] - 51s 619ms/step - loss: 0.0684 - accuracy: 0.5411 - val_loss: 0.0688 - val_accuracy: 0.5282\n",
      "Epoch 5/30\n",
      "83/83 [==============================] - 50s 598ms/step - loss: 0.0686 - accuracy: 0.5541 - val_loss: 0.0685 - val_accuracy: 0.5529\n",
      "Epoch 6/30\n",
      "83/83 [==============================] - 47s 569ms/step - loss: 0.0672 - accuracy: 0.5580 - val_loss: 0.0682 - val_accuracy: 0.5586\n",
      "Epoch 7/30\n",
      "83/83 [==============================] - 47s 563ms/step - loss: 0.0672 - accuracy: 0.5653 - val_loss: 0.0680 - val_accuracy: 0.5533\n",
      "Epoch 8/30\n",
      "83/83 [==============================] - 46s 560ms/step - loss: 0.0667 - accuracy: 0.5757 - val_loss: 0.0675 - val_accuracy: 0.5621\n",
      "Epoch 9/30\n",
      "83/83 [==============================] - 47s 570ms/step - loss: 0.0670 - accuracy: 0.5787 - val_loss: 0.0675 - val_accuracy: 0.5617\n",
      "Epoch 10/30\n",
      "83/83 [==============================] - 46s 557ms/step - loss: 0.0680 - accuracy: 0.5683 - val_loss: 0.0677 - val_accuracy: 0.5510\n",
      "Epoch 11/30\n",
      "83/83 [==============================] - 53s 636ms/step - loss: 0.0657 - accuracy: 0.5688 - val_loss: 0.0670 - val_accuracy: 0.5724\n",
      "Epoch 12/30\n",
      "83/83 [==============================] - 53s 635ms/step - loss: 0.0658 - accuracy: 0.5764 - val_loss: 0.0685 - val_accuracy: 0.5434\n",
      "Epoch 13/30\n",
      "83/83 [==============================] - 48s 574ms/step - loss: 0.0670 - accuracy: 0.5822 - val_loss: 0.0673 - val_accuracy: 0.5583\n",
      "Epoch 14/30\n",
      "83/83 [==============================] - 48s 578ms/step - loss: 0.0657 - accuracy: 0.5879 - val_loss: 0.0669 - val_accuracy: 0.5678\n",
      "Epoch 15/30\n",
      "83/83 [==============================] - 47s 568ms/step - loss: 0.0657 - accuracy: 0.5810 - val_loss: 0.0671 - val_accuracy: 0.5838\n",
      "Epoch 16/30\n",
      "83/83 [==============================] - 47s 570ms/step - loss: 0.0669 - accuracy: 0.5814 - val_loss: 0.0670 - val_accuracy: 0.5655\n",
      "Epoch 17/30\n",
      "83/83 [==============================] - 47s 568ms/step - loss: 0.0652 - accuracy: 0.5807 - val_loss: 0.0668 - val_accuracy: 0.5701\n",
      "Epoch 18/30\n",
      "83/83 [==============================] - 47s 566ms/step - loss: 0.0658 - accuracy: 0.5811 - val_loss: 0.0669 - val_accuracy: 0.5625\n",
      "Epoch 19/30\n",
      "83/83 [==============================] - 48s 574ms/step - loss: 0.0657 - accuracy: 0.5806 - val_loss: 0.0665 - val_accuracy: 0.5613\n",
      "Epoch 20/30\n",
      "83/83 [==============================] - 48s 573ms/step - loss: 0.0651 - accuracy: 0.5950 - val_loss: 0.0671 - val_accuracy: 0.5621\n",
      "Epoch 21/30\n",
      "83/83 [==============================] - 47s 566ms/step - loss: 0.0654 - accuracy: 0.5759 - val_loss: 0.0671 - val_accuracy: 0.5312\n",
      "Epoch 22/30\n",
      "83/83 [==============================] - 50s 600ms/step - loss: 0.0646 - accuracy: 0.5895 - val_loss: 0.0666 - val_accuracy: 0.5472\n",
      "Epoch 23/30\n",
      "83/83 [==============================] - 51s 621ms/step - loss: 0.0652 - accuracy: 0.5891 - val_loss: 0.0667 - val_accuracy: 0.5560\n",
      "Epoch 24/30\n",
      "83/83 [==============================] - 51s 612ms/step - loss: 0.0647 - accuracy: 0.5844 - val_loss: 0.0660 - val_accuracy: 0.5640\n",
      "Epoch 25/30\n",
      "83/83 [==============================] - 52s 631ms/step - loss: 0.0638 - accuracy: 0.5923 - val_loss: 0.0667 - val_accuracy: 0.5567\n",
      "Epoch 26/30\n",
      "83/83 [==============================] - 52s 624ms/step - loss: 0.0642 - accuracy: 0.5916 - val_loss: 0.0661 - val_accuracy: 0.5758\n",
      "Epoch 27/30\n",
      "83/83 [==============================] - 54s 653ms/step - loss: 0.0632 - accuracy: 0.6055 - val_loss: 0.0660 - val_accuracy: 0.5541\n",
      "Epoch 28/30\n",
      "83/83 [==============================] - 52s 632ms/step - loss: 0.0645 - accuracy: 0.5841 - val_loss: 0.0665 - val_accuracy: 0.5407\n",
      "Epoch 29/30\n",
      "83/83 [==============================] - 51s 608ms/step - loss: 0.0628 - accuracy: 0.6033 - val_loss: 0.0650 - val_accuracy: 0.5762\n",
      "Epoch 30/30\n",
      "83/83 [==============================] - 64s 775ms/step - loss: 0.0623 - accuracy: 0.6064 - val_loss: 0.0652 - val_accuracy: 0.5605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb29ef1a7b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "# This step may take 20-30 minutes. It may be easier to change epochs to 10 to save time.\n",
    "model.fit(image_train, bbox_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 60ms/step - loss: 0.0653 - accuracy: 0.5689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06526488810777664, 0.5688604712486267]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure accuracy against validation set.\n",
    "model.evaluate(image_test, bbox_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
